{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps: Experiment Tracking & Model Management\n",
    "\n",
    "This notebook demonstrates MLOps best practices:\n",
    "- MLflow experiment tracking\n",
    "- Model registry and versioning\n",
    "- A/B testing framework\n",
    "- Model comparison and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path(\"../src\").resolve()))\n",
    "\n",
    "from fittrack.data.ingestion import HARDataLoader, ACTIVITY_LABELS\n",
    "from fittrack.data.preprocessing import create_train_val_test_split\n",
    "from fittrack.models.classical import train_random_forest, train_xgboost, ModelConfig\n",
    "from fittrack.models.evaluation import compute_metrics\n",
    "from fittrack.mlops.tracking import (\n",
    "    ExperimentTracker,\n",
    "    log_training_run,\n",
    "    setup_mlflow,\n",
    "    start_run,\n",
    ")\n",
    "from fittrack.mlops.registry import ModelRegistry, promote_best_to_production\n",
    "from fittrack.mlops.ab_testing import (\n",
    "    ABTest,\n",
    "    run_offline_ab_test,\n",
    "    compute_sample_size,\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "FIGURES_DIR = Path(\"../docs/figures\")\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set MLflow tracking URI (local directory)\n",
    "MLFLOW_DIR = Path(\"../mlruns\")\n",
    "MLFLOW_DIR.mkdir(exist_ok=True)\n",
    "mlflow.set_tracking_uri(str(MLFLOW_DIR))\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "loader = HARDataLoader()\n",
    "train_data, test_data = loader.load_all()\n",
    "\n",
    "split = create_train_val_test_split(\n",
    "    train_data.X,\n",
    "    train_data.y,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    normalize=True,\n",
    ")\n",
    "\n",
    "X_test = split.scaler.transform(test_data.X.values)\n",
    "y_test = split.label_encoder.transform(test_data.y[\"activity\"])\n",
    "\n",
    "class_names = split.class_names\n",
    "print(f\"Train: {len(split.X_train)}, Val: {len(split.X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Experiment Tracking\n",
    "\n",
    "Track experiments with parameters, metrics, and artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment tracker\n",
    "tracker = ExperimentTracker(experiment_name=\"har-classification\")\n",
    "print(f\"Experiment ID: {tracker.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and log Random Forest\n",
    "with tracker.start_run(run_name=\"random-forest-baseline\", tags={\"model_type\": \"random_forest\"}) as run:\n",
    "    # Parameters\n",
    "    rf_params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": None,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"class_weight\": \"balanced\",\n",
    "    }\n",
    "    tracker.log_params(rf_params)\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = RandomForestClassifier(**rf_params, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(split.X_train, split.y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    y_proba = rf_model.predict_proba(X_test)\n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba, class_names)\n",
    "    \n",
    "    # Log metrics\n",
    "    tracker.log_metrics(metrics.to_dict())\n",
    "    \n",
    "    # Log model\n",
    "    tracker.log_model(rf_model, \"sklearn\", registered_model_name=\"har-classifier\")\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Accuracy: {metrics.accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {metrics.f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and log XGBoost\n",
    "with tracker.start_run(run_name=\"xgboost-baseline\", tags={\"model_type\": \"xgboost\"}) as run:\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # Parameters\n",
    "    xgb_params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "    }\n",
    "    tracker.log_params(xgb_params)\n",
    "    \n",
    "    # Train model\n",
    "    xgb_model = XGBClassifier(\n",
    "        **xgb_params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    )\n",
    "    xgb_model.fit(split.X_train, split.y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_proba = xgb_model.predict_proba(X_test)\n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba, class_names)\n",
    "    \n",
    "    # Log metrics\n",
    "    tracker.log_metrics(metrics.to_dict())\n",
    "    \n",
    "    # Log model\n",
    "    tracker.log_model(xgb_model, \"xgboost\", registered_model_name=\"har-classifier\")\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Accuracy: {metrics.accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {metrics.f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all runs\n",
    "comparison = tracker.compare_runs(metrics=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"])\n",
    "comparison_df = pd.DataFrame(comparison).T\n",
    "print(\"\\nRun Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run\n",
    "best_run = tracker.get_best_run(metric=\"f1_macro\")\n",
    "if best_run:\n",
    "    print(f\"\\nBest Run:\")\n",
    "    print(f\"  Name: {best_run['run_name']}\")\n",
    "    print(f\"  F1 Score: {best_run['metrics'].get('f1_macro', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Registry\n",
    "\n",
    "Manage model versions and lifecycle stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize registry\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Get all versions of our model\n",
    "try:\n",
    "    versions = registry.get_all_versions(\"har-classifier\")\n",
    "    print(f\"\\nRegistered model versions ({len(versions)}):\")\n",
    "    for v in versions:\n",
    "        print(f\"  v{v.version}: stage={v.stage}, created={v.creation_time}\")\n",
    "except Exception as e:\n",
    "    print(f\"No registered models yet: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model versions\n",
    "try:\n",
    "    version_comparison = registry.compare_versions(\"har-classifier\")\n",
    "    print(\"\\nVersion Comparison:\")\n",
    "    for version, data in version_comparison.items():\n",
    "        print(f\"\\nVersion {version} ({data['stage']}):\")\n",
    "        print(f\"  Accuracy: {data['metrics'].get('accuracy', 'N/A')}\")\n",
    "        print(f\"  F1: {data['metrics'].get('f1_macro', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compare versions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote best model to production\n",
    "try:\n",
    "    # This would promote based on F1 score\n",
    "    # promoted_version = promote_best_to_production(\"har-classifier\", metric=\"f1_macro\")\n",
    "    # print(f\"Promoted version {promoted_version} to Production\")\n",
    "    \n",
    "    # For demo, manually promote version 1\n",
    "    # registry.transition_stage(\"har-classifier\", 1, \"Production\")\n",
    "    print(\"Model promotion would happen here in a real deployment\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A/B Testing Framework\n",
    "\n",
    "Compare model variants with statistical rigor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate required sample size for A/B test\n",
    "baseline_accuracy = 0.90  # Expected accuracy\n",
    "mde = 0.02  # Minimum detectable effect (2% relative improvement)\n",
    "\n",
    "required_samples = compute_sample_size(\n",
    "    baseline_rate=baseline_accuracy,\n",
    "    minimum_detectable_effect=mde,\n",
    "    significance_level=0.05,\n",
    "    power=0.80,\n",
    ")\n",
    "\n",
    "print(f\"Required samples per variant: {required_samples:,}\")\n",
    "print(f\"Total samples needed: {required_samples * 2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run offline A/B test comparing RF vs XGBoost\n",
    "ab_result = run_offline_ab_test(\n",
    "    control_model=rf_model,\n",
    "    treatment_model=xgb_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    test_name=\"rf-vs-xgboost\",\n",
    "    control_name=\"random_forest\",\n",
    "    treatment_name=\"xgboost\",\n",
    ")\n",
    "\n",
    "print(ab_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize A/B test result\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "variants = [ab_result.control_variant, ab_result.treatment_variant]\n",
    "accuracies = [ab_result.control_value, ab_result.treatment_value]\n",
    "colors = [\"steelblue\", \"coral\"]\n",
    "\n",
    "bars = ax.bar(variants, accuracies, color=colors)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f\"{acc:.4f}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.set_title(f\"A/B Test: {ab_result.test_name}\\n\"\n",
    "             f\"(p-value: {ab_result.p_value:.4f}, \"\n",
    "             f\"significant: {ab_result.is_significant})\", fontsize=12)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add improvement annotation\n",
    "ax.annotate(\n",
    "    f\"{ab_result.relative_improvement:+.2%}\",\n",
    "    xy=(1, ab_result.treatment_value),\n",
    "    xytext=(1.3, ab_result.treatment_value),\n",
    "    fontsize=14,\n",
    "    color=\"green\" if ab_result.relative_improvement > 0 else \"red\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"ab_test_result.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive A/B test simulation\n",
    "ab_test = ABTest(\"simulated-test\", confidence_level=0.95)\n",
    "ab_test.add_variant(\"control\", rf_model, traffic_weight=0.5)\n",
    "ab_test.add_variant(\"treatment\", xgb_model, traffic_weight=0.5)\n",
    "ab_test.start()\n",
    "\n",
    "# Simulate traffic (using test data)\n",
    "n_simulate = min(500, len(X_test))\n",
    "for i in range(n_simulate):\n",
    "    sample_id = f\"user_{i}\"\n",
    "    variant, pred = ab_test.predict(X_test[i:i+1], sample_id=sample_id)\n",
    "    ab_test.record_outcome(sample_id, y_test[i])\n",
    "\n",
    "# Analyze\n",
    "simulated_result = ab_test.analyze(metric=\"accuracy\", control=\"control\")\n",
    "print(\"\\nSimulated A/B Test:\")\n",
    "print(simulated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summary\n",
    "summary = ab_test.get_summary()\n",
    "print(\"\\nTest Summary:\")\n",
    "print(f\"  Status: {summary['status']}\")\n",
    "print(f\"  Variants:\")\n",
    "for name, data in summary['variants'].items():\n",
    "    print(f\"    {name}: {data['n_predictions']} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLflow UI\n",
    "\n",
    "To view the MLflow UI, run in terminal:\n",
    "```bash\n",
    "cd fittrack-ml\n",
    "mlflow ui --backend-store-uri mlruns\n",
    "```\n",
    "\n",
    "Then open http://localhost:5000 in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### MLOps Best Practices Demonstrated:\n",
    "\n",
    "1. **Experiment Tracking** (MLflow)\n",
    "   - Log parameters, metrics, and models\n",
    "   - Compare runs and find best performers\n",
    "   - Reproducibility through logged artifacts\n",
    "\n",
    "2. **Model Registry**\n",
    "   - Version control for models\n",
    "   - Lifecycle stages (Staging \u2192 Production)\n",
    "   - Easy model loading for deployment\n",
    "\n",
    "3. **A/B Testing**\n",
    "   - Statistical significance testing\n",
    "   - Sample size calculation\n",
    "   - Traffic splitting strategies\n",
    "\n",
    "### Production Checklist:\n",
    "- [ ] Set up remote MLflow tracking server\n",
    "- [ ] Configure model registry backend (PostgreSQL, MySQL)\n",
    "- [ ] Implement model serving infrastructure\n",
    "- [ ] Set up monitoring and alerting\n",
    "- [ ] Automate model retraining pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}