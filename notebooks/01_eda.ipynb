{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: UCI HAR Dataset\n",
    "\n",
    "This notebook explores the UCI Human Activity Recognition dataset to understand:\n",
    "- Dataset structure and class distribution\n",
    "- Feature statistics and correlations\n",
    "- Time-series patterns per activity class\n",
    "\n",
    "**Dataset**: UCI HAR (30 subjects, 6 activities, 561 features from accelerometer/gyroscope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path(\"../src\").resolve()))\n",
    "\n",
    "from fittrack.data.ingestion import HARDataLoader, ACTIVITY_LABELS\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Output directory for figures\n",
    "FIGURES_DIR = Path(\"../docs/figures\")\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both train and test splits\n",
    "loader = HARDataLoader()\n",
    "train_data, test_data = loader.load_all()\n",
    "\n",
    "print(f\"Training samples: {train_data.n_samples:,}\")\n",
    "print(f\"Test samples: {test_data.n_samples:,}\")\n",
    "print(f\"Number of features: {train_data.n_features}\")\n",
    "print(f\"Number of classes: {train_data.n_classes}\")\n",
    "print(f\"\\nActivity labels: {list(ACTIVITY_LABELS.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine for overall analysis\n",
    "X_all = pd.concat([train_data.X, test_data.X], ignore_index=True)\n",
    "y_all = pd.concat([train_data.y, test_data.y], ignore_index=True)\n",
    "subject_ids_all = np.concatenate([train_data.subject_ids, test_data.subject_ids])\n",
    "\n",
    "print(f\"Total samples: {len(X_all):,}\")\n",
    "print(f\"Unique subjects: {len(np.unique(subject_ids_all))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = y_all[\"activity\"].value_counts()\n",
    "print(\"Class distribution:\")\n",
    "print(class_counts)\n",
    "print(f\"\\nClass balance ratio (max/min): {class_counts.max() / class_counts.min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = sns.color_palette(\"husl\", n_colors=6)\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(class_counts.index, class_counts.values, color=colors)\n",
    "ax1.set_xlabel(\"Activity\", fontsize=12)\n",
    "ax1.set_ylabel(\"Number of Samples\", fontsize=12)\n",
    "ax1.set_title(\"Class Distribution (All Data)\", fontsize=14)\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "             f\"{count:,}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct=\"%1.1f%%\",\n",
    "        colors=colors, startangle=90)\n",
    "ax2.set_title(\"Activity Distribution (%)\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"class_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'class_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vs Test split distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_counts = train_data.y[\"activity\"].value_counts()\n",
    "test_counts = test_data.y[\"activity\"].value_counts()\n",
    "\n",
    "x = np.arange(len(ACTIVITY_LABELS))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, train_counts.reindex(ACTIVITY_LABELS.values()).values,\n",
    "        width, label=\"Train\", color=\"steelblue\")\n",
    "ax1.bar(x + width/2, test_counts.reindex(ACTIVITY_LABELS.values()).values,\n",
    "        width, label=\"Test\", color=\"coral\")\n",
    "ax1.set_xlabel(\"Activity\", fontsize=12)\n",
    "ax1.set_ylabel(\"Number of Samples\", fontsize=12)\n",
    "ax1.set_title(\"Train vs Test Distribution\", fontsize=14)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(list(ACTIVITY_LABELS.values()), rotation=45, ha=\"right\")\n",
    "ax1.legend()\n",
    "\n",
    "# Subject distribution per activity\n",
    "ax2 = axes[1]\n",
    "subject_activity = pd.DataFrame({\"subject\": subject_ids_all, \"activity\": y_all[\"activity\"]})\n",
    "subject_counts = subject_activity.groupby(\"activity\")[\"subject\"].nunique()\n",
    "ax2.bar(subject_counts.index, subject_counts.values, color=colors)\n",
    "ax2.set_xlabel(\"Activity\", fontsize=12)\n",
    "ax2.set_ylabel(\"Number of Unique Subjects\", fontsize=12)\n",
    "ax2.set_title(\"Subjects per Activity\", fontsize=14)\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"train_test_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Feature statistics summary:\")\n",
    "stats_summary = X_all.describe().T\n",
    "print(f\"\\nFeature value ranges:\")\n",
    "print(f\"  Global min: {stats_summary['min'].min():.4f}\")\n",
    "print(f\"  Global max: {stats_summary['max'].max():.4f}\")\n",
    "print(f\"  Mean of means: {stats_summary['mean'].mean():.4f}\")\n",
    "print(f\"  Mean of stds: {stats_summary['std'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature naming patterns\n",
    "feature_names = X_all.columns.tolist()\n",
    "\n",
    "# Categorize features\n",
    "time_domain = [f for f in feature_names if f.startswith(\"t\")]\n",
    "freq_domain = [f for f in feature_names if f.startswith(\"f\")]\n",
    "\n",
    "body_acc = [f for f in feature_names if \"BodyAcc\" in f]\n",
    "gravity_acc = [f for f in feature_names if \"GravityAcc\" in f]\n",
    "body_gyro = [f for f in feature_names if \"BodyGyro\" in f]\n",
    "\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"  Time-domain features: {len(time_domain)}\")\n",
    "print(f\"  Frequency-domain features: {len(freq_domain)}\")\n",
    "print(f\"\\nSensor types:\")\n",
    "print(f\"  Body acceleration: {len(body_acc)}\")\n",
    "print(f\"  Gravity acceleration: {len(gravity_acc)}\")\n",
    "print(f\"  Body gyroscope: {len(body_gyro)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "key_features = [\n",
    "    \"tBodyAcc-mean()-X\",\n",
    "    \"tBodyAcc-mean()-Y\",\n",
    "    \"tBodyAcc-mean()-Z\",\n",
    "    \"tGravityAcc-mean()-X\",\n",
    "    \"tBodyGyro-mean()-X\",\n",
    "    \"tBodyAccMag-mean()\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    ax = axes[i]\n",
    "    for activity in ACTIVITY_LABELS.values():\n",
    "        mask = y_all[\"activity\"] == activity\n",
    "        ax.hist(X_all.loc[mask, feature], bins=50, alpha=0.5, label=activity, density=True)\n",
    "    ax.set_xlabel(feature, fontsize=10)\n",
    "    ax.set_ylabel(\"Density\", fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Feature Distributions by Activity Class\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"feature_distributions.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of features for correlation heatmap (too many for full matrix)\n",
    "# Focus on mean and std of primary signals\n",
    "corr_features = [\n",
    "    \"tBodyAcc-mean()-X\", \"tBodyAcc-mean()-Y\", \"tBodyAcc-mean()-Z\",\n",
    "    \"tBodyAcc-std()-X\", \"tBodyAcc-std()-Y\", \"tBodyAcc-std()-Z\",\n",
    "    \"tGravityAcc-mean()-X\", \"tGravityAcc-mean()-Y\", \"tGravityAcc-mean()-Z\",\n",
    "    \"tBodyGyro-mean()-X\", \"tBodyGyro-mean()-Y\", \"tBodyGyro-mean()-Z\",\n",
    "    \"tBodyGyro-std()-X\", \"tBodyGyro-std()-Y\", \"tBodyGyro-std()-Z\",\n",
    "    \"tBodyAccMag-mean()\", \"tBodyAccMag-std()\",\n",
    "    \"tGravityAccMag-mean()\", \"tGravityAccMag-std()\",\n",
    "    \"tBodyGyroMag-mean()\", \"tBodyGyroMag-std()\",\n",
    "]\n",
    "\n",
    "corr_matrix = X_all[corr_features].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap=\"RdBu_r\", center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "ax.set_title(\"Feature Correlation Heatmap (Selected Features)\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"correlation_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated feature pairs\n",
    "corr_full = X_all.corr()\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_full.columns)):\n",
    "    for j in range(i + 1, len(corr_full.columns)):\n",
    "        if abs(corr_full.iloc[i, j]) > 0.95:\n",
    "            high_corr_pairs.append((\n",
    "                corr_full.columns[i],\n",
    "                corr_full.columns[j],\n",
    "                corr_full.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"Number of highly correlated pairs (|r| > 0.95): {len(high_corr_pairs)}\")\n",
    "print(\"\\nSample highly correlated pairs:\")\n",
    "for f1, f2, corr in high_corr_pairs[:5]:\n",
    "    print(f\"  {f1} <-> {f2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activity-Specific Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of key features by activity\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "analysis_features = [\n",
    "    \"tBodyAcc-mean()-X\",\n",
    "    \"tGravityAcc-mean()-X\",\n",
    "    \"tBodyAccMag-std()\",\n",
    "    \"tBodyGyro-std()-X\",\n",
    "    \"tBodyAccJerk-mean()-X\",\n",
    "    \"fBodyAcc-meanFreq()-X\",\n",
    "]\n",
    "\n",
    "plot_df = X_all[analysis_features].copy()\n",
    "plot_df[\"activity\"] = y_all[\"activity\"].values\n",
    "\n",
    "for i, feature in enumerate(analysis_features):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(data=plot_df, x=\"activity\", y=feature, ax=ax, palette=\"husl\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(feature, fontsize=10)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.suptitle(\"Feature Distributions by Activity (Box Plots)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"feature_boxplots.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean feature values per activity (heatmap)\n",
    "# Select top features with highest variance across activities\n",
    "activity_means = X_all.groupby(y_all[\"activity\"]).mean()\n",
    "feature_variance = activity_means.var()\n",
    "top_varying_features = feature_variance.nlargest(30).index.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(activity_means[top_varying_features].T, cmap=\"RdBu_r\", center=0,\n",
    "            annot=False, ax=ax, cbar_kws={\"label\": \"Mean Value\"})\n",
    "ax.set_xlabel(\"Activity\", fontsize=12)\n",
    "ax.set_ylabel(\"Feature\", fontsize=12)\n",
    "ax.set_title(\"Mean Feature Values by Activity (Top 30 Most Varying Features)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"activity_feature_means.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Time-Series Patterns\n",
    "\n",
    "While the UCI HAR dataset provides pre-extracted features (not raw time-series), we can visualize samples across feature dimensions to understand activity signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample profiles per activity\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "acc_features = [f for f in feature_names if \"tBodyAcc-\" in f and \"Jerk\" not in f][:20]\n",
    "\n",
    "for i, activity in enumerate(ACTIVITY_LABELS.values()):\n",
    "    ax = axes[i]\n",
    "    mask = y_all[\"activity\"] == activity\n",
    "    samples = X_all.loc[mask, acc_features].iloc[:5]  # 5 random samples\n",
    "    \n",
    "    for j, (idx, row) in enumerate(samples.iterrows()):\n",
    "        ax.plot(range(len(acc_features)), row.values, alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(activity, fontsize=12)\n",
    "    ax.set_xlabel(\"Feature Index\", fontsize=10)\n",
    "    ax.set_ylabel(\"Value\", fontsize=10)\n",
    "    ax.set_ylim(-1, 1)\n",
    "\n",
    "plt.suptitle(\"Sample Feature Profiles per Activity (Body Acceleration Features)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"activity_profiles.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar/spider chart for activity fingerprints\n",
    "from math import pi\n",
    "\n",
    "# Select representative features\n",
    "radar_features = [\n",
    "    \"tBodyAcc-mean()-X\", \"tBodyAcc-mean()-Y\", \"tBodyAcc-mean()-Z\",\n",
    "    \"tBodyAcc-std()-X\", \"tGravityAcc-mean()-X\", \"tBodyGyro-std()-X\",\n",
    "    \"tBodyAccMag-mean()\", \"tBodyGyroMag-std()\"\n",
    "]\n",
    "\n",
    "# Normalize for radar chart\n",
    "radar_df = X_all[radar_features].copy()\n",
    "radar_df = (radar_df - radar_df.min()) / (radar_df.max() - radar_df.min())\n",
    "radar_df[\"activity\"] = y_all[\"activity\"].values\n",
    "activity_profiles = radar_df.groupby(\"activity\").mean()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "angles = [n / float(len(radar_features)) * 2 * pi for n in range(len(radar_features))]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "for activity in ACTIVITY_LABELS.values():\n",
    "    values = activity_profiles.loc[activity].values.tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, linewidth=2, label=activity)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([f.replace(\"tBody\", \"\").replace(\"tGravity\", \"G-\")[:15] \n",
    "                    for f in radar_features], size=9)\n",
    "ax.set_title(\"Activity Fingerprints (Normalized Feature Profiles)\", fontsize=14, y=1.08)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"activity_radar.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_all)\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"  PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"  PC2: {pca.explained_variance_ratio_[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PCA plot\n",
    "ax1 = axes[0]\n",
    "for activity in ACTIVITY_LABELS.values():\n",
    "    mask = y_all[\"activity\"] == activity\n",
    "    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], label=activity, alpha=0.5, s=10)\n",
    "ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\", fontsize=12)\n",
    "ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\", fontsize=12)\n",
    "ax1.set_title(\"PCA Projection\", fontsize=14)\n",
    "ax1.legend(markerscale=3)\n",
    "\n",
    "# t-SNE (subsample for speed)\n",
    "np.random.seed(42)\n",
    "subsample_idx = np.random.choice(len(X_all), size=3000, replace=False)\n",
    "X_sub = X_all.iloc[subsample_idx]\n",
    "y_sub = y_all.iloc[subsample_idx]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_sub)\n",
    "\n",
    "ax2 = axes[1]\n",
    "for activity in ACTIVITY_LABELS.values():\n",
    "    mask = y_sub[\"activity\"] == activity\n",
    "    ax2.scatter(X_tsne[mask, 0], X_tsne[mask, 1], label=activity, alpha=0.5, s=10)\n",
    "ax2.set_xlabel(\"t-SNE 1\", fontsize=12)\n",
    "ax2.set_ylabel(\"t-SNE 2\", fontsize=12)\n",
    "ax2.set_title(\"t-SNE Projection (3000 samples)\", fontsize=14)\n",
    "ax2.legend(markerscale=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"dimensionality_reduction.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"UCI HAR DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset size:\")\n",
    "print(f\"  Total samples: {len(X_all):,}\")\n",
    "print(f\"  Training samples: {train_data.n_samples:,} ({train_data.n_samples/len(X_all):.1%})\")\n",
    "print(f\"  Test samples: {test_data.n_samples:,} ({test_data.n_samples/len(X_all):.1%})\")\n",
    "print(f\"\\nFeatures:\")\n",
    "print(f\"  Total features: {train_data.n_features}\")\n",
    "print(f\"  Time-domain: {len(time_domain)}\")\n",
    "print(f\"  Frequency-domain: {len(freq_domain)}\")\n",
    "print(f\"  Highly correlated pairs (|r|>0.95): {len(high_corr_pairs)}\")\n",
    "print(f\"\\nClasses:\")\n",
    "print(f\"  Number of activities: {train_data.n_classes}\")\n",
    "print(f\"  Activities: {list(ACTIVITY_LABELS.values())}\")\n",
    "print(f\"  Class balance (max/min): {class_counts.max() / class_counts.min():.2f}\")\n",
    "print(f\"\\nSubjects:\")\n",
    "print(f\"  Total subjects: {len(np.unique(subject_ids_all))}\")\n",
    "print(f\"\\nKey observations:\")\n",
    "print(\"  - Dataset is relatively balanced across classes\")\n",
    "print(\"  - High feature correlations suggest dimensionality reduction may help\")\n",
    "print(\"  - Clear separation visible between static (sitting/standing/laying) and dynamic activities\")\n",
    "print(\"  - PCA captures ~50% variance in first 2 components\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved figures\n",
    "print(\"\\nSaved figures:\")\n",
    "for fig_path in sorted(FIGURES_DIR.glob(\"*.png\")):\n",
    "    print(f\"  - {fig_path.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
