{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Activity Recognition\n",
    "\n",
    "This notebook trains and evaluates deep learning models:\n",
    "- LSTM (Long Short-Term Memory)\n",
    "- 1D-CNN (Convolutional Neural Network)\n",
    "- MLP (Multi-Layer Perceptron) baseline\n",
    "\n",
    "We compare with classical ML results from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path(\"../src\").resolve()))\n",
    "\n",
    "from fittrack.data.ingestion import HARDataLoader, ACTIVITY_LABELS\n",
    "from fittrack.data.preprocessing import create_train_val_test_split\n",
    "from fittrack.models.data_loaders import (\n",
    "    create_data_loaders,\n",
    "    get_device,\n",
    "    DataModule,\n",
    "    reshape_for_sequence_model,\n",
    ")\n",
    "from fittrack.models.deep_learning import (\n",
    "    ActivityLSTM,\n",
    "    ActivityCNN,\n",
    "    HARClassifier,\n",
    "    TrainingConfig,\n",
    "    train_model,\n",
    "    predict,\n",
    "    create_model,\n",
    ")\n",
    "from fittrack.models.evaluation import (\n",
    "    compute_metrics,\n",
    "    plot_confusion_matrix,\n",
    "    plot_model_comparison,\n",
    "    ModelEvaluator,\n",
    ")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "FIGURES_DIR = Path(\"../docs/figures\")\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = HARDataLoader()\n",
    "train_data, test_data = loader.load_all()\n",
    "\n",
    "print(f\"Training samples: {train_data.n_samples}\")\n",
    "print(f\"Test samples: {test_data.n_samples}\")\n",
    "print(f\"Features: {train_data.n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val split\n",
    "split = create_train_val_test_split(\n",
    "    train_data.X,\n",
    "    train_data.y,\n",
    "    val_size=0.15,\n",
    "    test_size=0.0,\n",
    "    normalize=True,\n",
    ")\n",
    "\n",
    "# Prepare test set\n",
    "X_test = split.scaler.transform(test_data.X.values)\n",
    "y_test = split.label_encoder.transform(test_data.y[\"activity\"])\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(split.X_train)}\")\n",
    "print(f\"  Validation: {len(split.X_val)}\")\n",
    "print(f\"  Test: {len(X_test)}\")\n",
    "\n",
    "class_names = split.class_names\n",
    "n_classes = split.n_classes\n",
    "n_features = split.n_features\n",
    "print(f\"\\nClasses ({n_classes}): {class_names}\")\n",
    "print(f\"Features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "\n",
    "loaders = create_data_loaders(\n",
    "    split.X_train, split.y_train,\n",
    "    split.X_val, split.y_val,\n",
    "    X_test, y_test,\n",
    "    batch_size=batch_size,\n",
    "    weighted_sampling=True,  # Handle class imbalance\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(loaders['train'])}\")\n",
    "print(f\"Val batches: {len(loaders['val'])}\")\n",
    "print(f\"Test batches: {len(loaders['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP model\n",
    "mlp_model = HARClassifier(\n",
    "    input_size=n_features,\n",
    "    num_classes=n_classes,\n",
    "    hidden_sizes=[256, 128],\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "print(mlp_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "config = TrainingConfig(\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    patience=10,\n",
    "    scheduler=\"plateau\",\n",
    ")\n",
    "\n",
    "# Train MLP\n",
    "print(\"Training MLP...\")\n",
    "mlp_history = train_model(\n",
    "    mlp_model,\n",
    "    loaders[\"train\"],\n",
    "    loaders[\"val\"],\n",
    "    config=config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(mlp_history.train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(mlp_history.val_losses, label=\"Val Loss\")\n",
    "axes[0].axvline(mlp_history.best_epoch, color='r', linestyle='--', label=f\"Best Epoch ({mlp_history.best_epoch})\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"MLP - Training Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(mlp_history.train_accuracies, label=\"Train Acc\")\n",
    "axes[1].plot(mlp_history.val_accuracies, label=\"Val Acc\")\n",
    "axes[1].axvline(mlp_history.best_epoch, color='r', linestyle='--')\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"MLP - Training Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"mlp_training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(mlp_history.val_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Model\n",
    "\n",
    "For LSTM, we reshape the features to treat them as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for sequence model\n",
    "# Treat the 561 features as a sequence of length 561 with 1 channel\n",
    "# (In a real scenario with raw sensor data, you'd have time x channels)\n",
    "\n",
    "seq_length = n_features\n",
    "n_channels = 1\n",
    "\n",
    "X_train_seq = reshape_for_sequence_model(split.X_train, seq_length, n_channels)\n",
    "X_val_seq = reshape_for_sequence_model(split.X_val, seq_length, n_channels)\n",
    "X_test_seq = reshape_for_sequence_model(X_test, seq_length, n_channels)\n",
    "\n",
    "print(f\"Sequence shape: {X_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence data loaders\n",
    "seq_loaders = create_data_loaders(\n",
    "    X_train_seq, split.y_train,\n",
    "    X_val_seq, split.y_val,\n",
    "    X_test_seq, y_test,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "lstm_model = ActivityLSTM(\n",
    "    input_size=n_channels,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    num_classes=n_classes,\n",
    "    dropout=0.3,\n",
    "    bidirectional=True,\n",
    ")\n",
    "\n",
    "print(lstm_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "print(\"Training LSTM...\")\n",
    "lstm_config = TrainingConfig(\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    patience=10,\n",
    "    gradient_clip=1.0,\n",
    ")\n",
    "\n",
    "lstm_history = train_model(\n",
    "    lstm_model,\n",
    "    seq_loaders[\"train\"],\n",
    "    seq_loaders[\"val\"],\n",
    "    config=lstm_config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(lstm_history.train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(lstm_history.val_losses, label=\"Val Loss\")\n",
    "axes[0].axvline(lstm_history.best_epoch, color='r', linestyle='--', label=f\"Best Epoch ({lstm_history.best_epoch})\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"LSTM - Training Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(lstm_history.train_accuracies, label=\"Train Acc\")\n",
    "axes[1].plot(lstm_history.val_accuracies, label=\"Val Acc\")\n",
    "axes[1].axvline(lstm_history.best_epoch, color='r', linestyle='--')\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"LSTM - Training Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"lstm_training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(lstm_history.val_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 1D-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model\n",
    "cnn_model = ActivityCNN(\n",
    "    in_channels=n_channels,\n",
    "    num_classes=n_classes,\n",
    "    seq_length=seq_length,\n",
    "    channels=[32, 64, 128],\n",
    "    kernel_sizes=[7, 5, 3],\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "print(cnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "print(\"Training CNN...\")\n",
    "cnn_config = TrainingConfig(\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "cnn_history = train_model(\n",
    "    cnn_model,\n",
    "    seq_loaders[\"train\"],\n",
    "    seq_loaders[\"val\"],\n",
    "    config=cnn_config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(cnn_history.train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(cnn_history.val_losses, label=\"Val Loss\")\n",
    "axes[0].axvline(cnn_history.best_epoch, color='r', linestyle='--', label=f\"Best Epoch ({cnn_history.best_epoch})\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"CNN - Training Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(cnn_history.train_accuracies, label=\"Train Acc\")\n",
    "axes[1].plot(cnn_history.val_accuracies, label=\"Val Acc\")\n",
    "axes[1].axvline(cnn_history.best_epoch, color='r', linestyle='--')\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"CNN - Training Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"cnn_training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(cnn_history.val_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "evaluator = ModelEvaluator(class_names=class_names, figures_dir=FIGURES_DIR)\n",
    "\n",
    "# MLP (uses flat features)\n",
    "mlp_preds, mlp_probs = predict(mlp_model, loaders[\"test\"], device, return_probs=True)\n",
    "mlp_metrics = compute_metrics(y_test, mlp_preds, mlp_probs, class_names)\n",
    "\n",
    "# LSTM (uses sequence features)\n",
    "lstm_preds, lstm_probs = predict(lstm_model, seq_loaders[\"test\"], device, return_probs=True)\n",
    "lstm_metrics = compute_metrics(y_test, lstm_preds, lstm_probs, class_names)\n",
    "\n",
    "# CNN (uses sequence features)\n",
    "cnn_preds, cnn_probs = predict(cnn_model, seq_loaders[\"test\"], device, return_probs=True)\n",
    "cnn_metrics = compute_metrics(y_test, cnn_preds, cnn_probs, class_names)\n",
    "\n",
    "print(\"Test Set Results:\")\n",
    "print(f\"\\nMLP:\")\n",
    "print(mlp_metrics)\n",
    "print(f\"\\nLSTM:\")\n",
    "print(lstm_metrics)\n",
    "print(f\"\\nCNN:\")\n",
    "print(cnn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison plot\n",
    "plot_model_comparison(\n",
    "    {\"MLP\": mlp_metrics, \"LSTM\": lstm_metrics, \"CNN\": cnn_metrics},\n",
    "    metric_names=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n",
    "    title=\"Deep Learning Model Comparison\",\n",
    ")\n",
    "plt.savefig(FIGURES_DIR / \"deep_learning_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, metrics) in zip(axes, [(\"MLP\", mlp_metrics), (\"LSTM\", lstm_metrics), (\"CNN\", cnn_metrics)]):\n",
    "    plt.sca(ax)\n",
    "    plot_confusion_matrix(\n",
    "        metrics.confusion_matrix,\n",
    "        class_names,\n",
    "        title=f\"{name} - Confusion Matrix\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"deep_learning_confusion_matrices.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Validation Loss\n",
    "axes[0].plot(mlp_history.val_losses, label=\"MLP\")\n",
    "axes[0].plot(lstm_history.val_losses, label=\"LSTM\")\n",
    "axes[0].plot(cnn_history.val_losses, label=\"CNN\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Validation Loss\")\n",
    "axes[0].set_title(\"Validation Loss Comparison\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Validation Accuracy\n",
    "axes[1].plot(mlp_history.val_accuracies, label=\"MLP\")\n",
    "axes[1].plot(lstm_history.val_accuracies, label=\"LSTM\")\n",
    "axes[1].plot(cnn_history.val_accuracies, label=\"CNN\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "axes[1].set_title(\"Validation Accuracy Comparison\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"training_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = {\n",
    "    \"Model\": [\"MLP\", \"LSTM\", \"CNN\"],\n",
    "    \"Accuracy\": [mlp_metrics.accuracy, lstm_metrics.accuracy, cnn_metrics.accuracy],\n",
    "    \"F1 (Macro)\": [mlp_metrics.f1_macro, lstm_metrics.f1_macro, cnn_metrics.f1_macro],\n",
    "    \"Precision\": [mlp_metrics.precision_macro, lstm_metrics.precision_macro, cnn_metrics.precision_macro],\n",
    "    \"Recall\": [mlp_metrics.recall_macro, lstm_metrics.recall_macro, cnn_metrics.recall_macro],\n",
    "    \"Parameters\": [\n",
    "        sum(p.numel() for p in mlp_model.parameters()),\n",
    "        sum(p.numel() for p in lstm_model.parameters()),\n",
    "        sum(p.numel() for p in cnn_model.parameters()),\n",
    "    ],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.round(4)\n",
    "print(\"\\nModel Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model_name = summary_df.loc[summary_df[\"F1 (Macro)\"].idxmax(), \"Model\"]\n",
    "best_f1 = summary_df[\"F1 (Macro)\"].max()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"BEST DEEP LEARNING MODEL: {best_model_name}\")\n",
    "print(f\"F1 Score (Macro): {best_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **MLP** serves as a strong baseline for pre-computed features\n",
    "2. **LSTM** can capture sequential patterns in the feature sequence\n",
    "3. **CNN** learns local patterns effectively through convolutions\n",
    "4. The UCI HAR features are already highly engineered, so deep learning doesn't always outperform classical ML\n",
    "5. With raw sensor data (not pre-computed features), LSTM/CNN would show larger improvements\n",
    "\n",
    "### Next Steps\n",
    "- Implement MLflow tracking for experiment management\n",
    "- Set up A/B testing framework for model comparison\n",
    "- Deploy the best model as a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine best model and save\n",
    "if best_model_name == \"MLP\":\n",
    "    best_model = mlp_model\n",
    "elif best_model_name == \"LSTM\":\n",
    "    best_model = lstm_model\n",
    "else:\n",
    "    best_model = cnn_model\n",
    "\n",
    "torch.save(best_model.state_dict(), models_dir / f\"best_deep_learning_{best_model_name.lower()}.pt\")\n",
    "print(f\"Saved best model to {models_dir / f'best_deep_learning_{best_model_name.lower()}.pt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
