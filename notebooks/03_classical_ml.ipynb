{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Machine Learning for Activity Recognition\n",
    "\n",
    "This notebook trains and evaluates classical ML models:\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "We compare performance using cross-validation and comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path(\"../src\").resolve()))\n",
    "\n",
    "from fittrack.data.ingestion import HARDataLoader, ACTIVITY_LABELS\n",
    "from fittrack.data.preprocessing import (\n",
    "    create_train_val_test_split,\n",
    "    get_class_weights,\n",
    "    get_sample_weights,\n",
    ")\n",
    "from fittrack.models.classical import (\n",
    "    ModelConfig,\n",
    "    train_random_forest,\n",
    "    train_xgboost,\n",
    "    train_with_tuning,\n",
    "    cross_validate_model,\n",
    "    get_top_features,\n",
    "    ClassicalModelTrainer,\n",
    ")\n",
    "from fittrack.models.evaluation import (\n",
    "    compute_metrics,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curves,\n",
    "    plot_precision_recall_per_class,\n",
    "    plot_model_comparison,\n",
    "    print_classification_report,\n",
    "    ModelEvaluator,\n",
    ")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "FIGURES_DIR = Path(\"../docs/figures\")\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = HARDataLoader()\n",
    "train_data, test_data = loader.load_all()\n",
    "\n",
    "print(f\"Training samples: {train_data.n_samples}\")\n",
    "print(f\"Test samples: {test_data.n_samples}\")\n",
    "print(f\"Features: {train_data.n_features}\")\n",
    "print(f\"Classes: {train_data.n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test split from training data\n",
    "# (The original test set is held out for final evaluation)\n",
    "split = create_train_val_test_split(\n",
    "    train_data.X,\n",
    "    train_data.y,\n",
    "    val_size=0.15,\n",
    "    test_size=0.0,  # We use the original test set\n",
    "    normalize=True,\n",
    ")\n",
    "\n",
    "# Normalize the held-out test set using the same scaler\n",
    "X_test = split.scaler.transform(test_data.X.values)\n",
    "y_test = split.label_encoder.transform(test_data.y[\"activity\"])\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(split.X_train)}\")\n",
    "print(f\"  Validation: {len(split.X_val)}\")\n",
    "print(f\"  Test (held out): {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names and weights\n",
    "class_names = split.class_names\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Compute class weights for handling imbalance\n",
    "class_weights = get_class_weights(split.y_train)\n",
    "print(f\"\\nClass weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with default settings\n",
    "rf_config = ModelConfig(n_estimators=100, max_depth=None, random_state=42)\n",
    "rf_result = train_random_forest(\n",
    "    split.X_train, split.y_train,\n",
    "    split.X_val, split.y_val,\n",
    "    config=rf_config,\n",
    ")\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"  Train Accuracy: {rf_result.train_score:.4f}\")\n",
    "print(f\"  Validation Accuracy: {rf_result.val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "cv_scores = cross_validate_model(\n",
    "    rf_model,\n",
    "    split.X_train,\n",
    "    split.y_train,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "print(f\"\\n5-Fold CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train XGBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb_config = ModelConfig(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_result = train_xgboost(\n",
    "    split.X_train, split.y_train,\n",
    "    split.X_val, split.y_val,\n",
    "    config=xgb_config,\n",
    ")\n",
    "\n",
    "print(f\"\\nXGBoost Results:\")\n",
    "print(f\"  Train Accuracy: {xgb_result.train_score:.4f}\")\n",
    "print(f\"  Validation Accuracy: {xgb_result.val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Random Forest (quick search for demo)\n",
    "print(\"Tuning Random Forest...\")\n",
    "rf_tuned_result = train_with_tuning(\n",
    "    \"random_forest\",\n",
    "    split.X_train, split.y_train,\n",
    "    split.X_val, split.y_val,\n",
    "    search_type=\"random\",\n",
    "    cv=3,\n",
    "    n_iter=20,\n",
    ")\n",
    "\n",
    "print(f\"\\nTuned Random Forest:\")\n",
    "print(f\"  Best params: {rf_tuned_result.best_params}\")\n",
    "print(f\"  Train Accuracy: {rf_tuned_result.train_score:.4f}\")\n",
    "print(f\"  Validation Accuracy: {rf_tuned_result.val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune XGBoost\n",
    "print(\"Tuning XGBoost...\")\n",
    "xgb_tuned_result = train_with_tuning(\n",
    "    \"xgboost\",\n",
    "    split.X_train, split.y_train,\n",
    "    split.X_val, split.y_val,\n",
    "    search_type=\"random\",\n",
    "    cv=3,\n",
    "    n_iter=20,\n",
    ")\n",
    "\n",
    "print(f\"\\nTuned XGBoost:\")\n",
    "print(f\"  Best params: {xgb_tuned_result.best_params}\")\n",
    "print(f\"  Train Accuracy: {xgb_tuned_result.train_score:.4f}\")\n",
    "print(f\"  Validation Accuracy: {xgb_tuned_result.val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(class_names=class_names, figures_dir=FIGURES_DIR)\n",
    "\n",
    "# Evaluate both models\n",
    "rf_metrics = evaluator.evaluate(rf_tuned_result.model, X_test, y_test, \"Random Forest\")\n",
    "xgb_metrics = evaluator.evaluate(xgb_tuned_result.model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "print(evaluator.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST - Classification Report\")\n",
    "print(\"=\"*60)\n",
    "y_pred_rf = rf_tuned_result.model.predict(X_test)\n",
    "print_classification_report(y_test, y_pred_rf, class_names)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST - Classification Report\")\n",
    "print(\"=\"*60)\n",
    "y_pred_xgb = xgb_tuned_result.model.predict(X_test)\n",
    "print_classification_report(y_test, y_pred_xgb, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest\n",
    "plot_confusion_matrix(\n",
    "    rf_metrics.confusion_matrix,\n",
    "    class_names,\n",
    "    title=\"Random Forest - Confusion Matrix\",\n",
    ")\n",
    "plt.savefig(FIGURES_DIR / \"confusion_matrix_rf.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# XGBoost\n",
    "plot_confusion_matrix(\n",
    "    xgb_metrics.confusion_matrix,\n",
    "    class_names,\n",
    "    title=\"XGBoost - Confusion Matrix\",\n",
    ")\n",
    "plt.savefig(FIGURES_DIR / \"confusion_matrix_xgb.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "y_proba_rf = rf_tuned_result.model.predict_proba(X_test)\n",
    "y_proba_xgb = xgb_tuned_result.model.predict_proba(X_test)\n",
    "\n",
    "plot_roc_curves(y_test, y_proba_rf, class_names, title=\"Random Forest - ROC Curves\")\n",
    "plt.savefig(FIGURES_DIR / \"roc_curves_rf.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "plot_roc_curves(y_test, y_proba_xgb, class_names, title=\"XGBoost - ROC Curves\")\n",
    "plt.savefig(FIGURES_DIR / \"roc_curves_xgb.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "plot_model_comparison(\n",
    "    {\"Random Forest\": rf_metrics, \"XGBoost\": xgb_metrics},\n",
    "    metric_names=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n",
    "    title=\"Model Comparison on Test Set\",\n",
    ")\n",
    "plt.savefig(FIGURES_DIR / \"model_comparison_classical.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "plot_precision_recall_per_class(\n",
    "    rf_metrics,\n",
    "    class_names,\n",
    "    title=\"Random Forest - Per-Class Metrics\",\n",
    ")\n",
    "plt.savefig(FIGURES_DIR / \"per_class_metrics_rf.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features from Random Forest\n",
    "feature_names = train_data.X.columns.tolist()\n",
    "top_features_rf = get_top_features(\n",
    "    rf_tuned_result.feature_importances,\n",
    "    feature_names,\n",
    "    n_top=20,\n",
    ")\n",
    "\n",
    "print(\"Top 20 Features (Random Forest):\")\n",
    "for name, importance in top_features_rf:\n",
    "    print(f\"  {name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "names, importances = zip(*top_features_rf)\n",
    "y_pos = np.arange(len(names))\n",
    "\n",
    "ax.barh(y_pos, importances, color=\"steelblue\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(names)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"Feature Importance\")\n",
    "ax.set_title(\"Top 20 Features - Random Forest\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"feature_importance_rf.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Findings:\n",
    "- Both Random Forest and XGBoost achieve strong performance on the HAR dataset\n",
    "- The models show high accuracy on static activities (sitting, standing, laying)\n",
    "- Some confusion exists between similar activities (walking upstairs/downstairs)\n",
    "- Gravity acceleration and body acceleration features are most predictive\n",
    "\n",
    "### Next Steps:\n",
    "- Try deep learning models (LSTM, CNN) for potentially better temporal pattern recognition\n",
    "- Implement MLflow tracking for experiment management\n",
    "- Deploy the best model as a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if rf_metrics.f1_macro >= xgb_metrics.f1_macro:\n",
    "    print(f\"Best model: Random Forest\")\n",
    "    print(f\"F1 Score (macro): {rf_metrics.f1_macro:.4f}\")\n",
    "    print(f\"Accuracy: {rf_metrics.accuracy:.4f}\")\n",
    "    best_model = rf_tuned_result.model\n",
    "else:\n",
    "    print(f\"Best model: XGBoost\")\n",
    "    print(f\"F1 Score (macro): {xgb_metrics.f1_macro:.4f}\")\n",
    "    print(f\"Accuracy: {xgb_metrics.accuracy:.4f}\")\n",
    "    best_model = xgb_tuned_result.model\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
